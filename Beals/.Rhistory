install.packages('rJava',,'http://www.rforge.net/')
install.packages('rJava', .libPaths()[1], 'http://www.rforge.net/')
install.packages('rJava', repos='http://www.rforge.net/')
Sys.getenv("JAVA_HOME")
library(rJava)
if (Sys.getenv("JAVA_HOME")!="")
Sys.setenv(JAVA_HOME="")
library(rJava)
if (Sys.getenv("JAVA_HOME")!="")
Sys.setenv(JAVA_HOME="")
library(rJava)
if (Sys.getenv("JAVA_HOME")!="")
Sys.setenv(JAVA_HOME="")
library(rJava)
library(rJava)
print(this)
library("rJava", lib.loc="~/R/win-library/3.2")
library("mallet", lib.loc="~/R/win-library/3.2")
library("utils", lib.loc="C:/Program Files/R/R-3.2.4revised/library")
detach("package:rJava", unload=TRUE)
library("rJava", lib.loc="~/R/win-library/3.2")
Sys.getenv("JAVA_HOME")
Sys.setenv(JAVA_HOME="")
Sys.getenv("JAVA_HOME")
library("rJava", lib.loc="~/R/win-library/3.2")
library("rJava", lib.loc="~/R/win-library/3.2")
library("mallet", lib.loc="~/R/win-library/3.2")
setwd("C:\\Users\\hp\\Desktop\\Beals")
options(java.parameters = "-Xmx6000m")
install.packages('RCurl')
library("RCurl", lib.loc="~/R/win-library/3.2")
x <- getURL("https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv", .opts = list(ssl.verifypeer = FALSE))
documents <- read.csv(text = x, col.names=c("Article_ID", "Newspaper Title", "Newspaper City", "Newspaper Province", "Newspaper Country", "Year", "Month", "Day", "Article Type", "Text", "Keywords"), colClasses=rep("character", 3), sep=",", quote="")
counts <- table(documents$Newspaper.City)
barplot(counts, main="Cities", xlab="Number of Articles")
years <- table(documents$Year)
barplot(years, main="Publication Year", xlab="Year", ylab="Number of Articles")
mallet.instances <- mallet.import(documents$Article_ID, documents$Text, "C:\\Mallet\\stoplists\\en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
library("mallet", lib.loc="~/R/win-library/3.2")
mallet.instances <- mallet.import(documents$Article_ID, documents$Text, "C:\\Mallet\\stoplists\\en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
library("rJava", lib.loc="~/R/win-library/3.2")
detach("package:rJava", unload=TRUE)
detach("package:mallet", unload=TRUE)
library("rJava", lib.loc="~/R/win-library/3.2")
library("mallet", lib.loc="~/R/win-library/3.2")
mallet.instances <- mallet.import(documents$Article_ID, documents$Text, "C:\\Mallet\\stoplists\\en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances <- mallet.import(documents$Article_ID, documents$Text, "stoplist.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
num.topics <- 50
topic.model <- MalletLDA(num.topics)
topic.model$loadDocuments(mallet.instances)
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
head(word.freqs)
write.csv(word.freqs, "cnd-word-freqs.csv" )
## Optimize hyperparameters every 20 iterations,
## after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
## We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(1000)
## Run through a few iterations where we pick the best topic for each token,
## rather than sampling from the posterior distribution.
topic.model$maximize(10)
## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities,
## so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
mallet.top.words(topic.model, topic.words[7,])
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
write.csv(topic.docs, "cnd-topics-docs.csv" )
## Get a vector containing short names for the topics
topics.labels <- rep("", num.topics)
for (topic in 1:num.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
# have a look at keywords for each topic
topics.labels
write.csv(topics.labels, "cnd-topics-labels.csv")
install.packages('wordcloud')
library(wordcloud)
for(i in 1:num.topics){
topic.top.words <- mallet.top.words(topic.model,
topic.words[i,], 25)
print(wordcloud(topic.top.words$words,
topic.top.words$weights,
c(4,.8), rot.per=0,
random.order=F))
}
plot(hclust(dist(topic.words)), labels=topics.labels)
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$article_id
install.packages("cluster")
library(cluster)
topic_df_dist <- as.matrix(daisy(t(topic_docs), metric = "euclidean", stand = TRUE))
# Change row values to zero if less than row minimum plus row standard deviation
# keep only closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
topic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) > 0 ] <- 0
install.packages("igraph")
library(igraph)
g <- as.undirected(graph.adjacency(topic_df_dist))
layout1 <- layout.fruchterman.reingold(g, niter=500)
plot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color= "grey", edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)
write.graph(g, file="cnd.graphml", format="graphml")
savehistory("C:/Users/hp/Desktop/Beals/.Rhistory")
